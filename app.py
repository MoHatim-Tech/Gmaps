import streamlit as st
import pandas as pd
from playwright.sync_api import sync_playwright
import time
import re
import requests
from bs4 import BeautifulSoup
import asyncio
import sys
from docx import Document
from docx.shared import Pt
from docx.enum.text import WD_ALIGN_PARAGRAPH
import io
import subprocess

# ÙˆØ¸ÙŠÙØ© Ù„ØªØ«Ø¨ÙŠØª Ù…ØªØµÙØ­ Playwright ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ±ÙØ±
def install_playwright_browsers():
    try:
        subprocess.run(["playwright", "install", "chromium"], check=True)
    except Exception as e:
        st.error(f"Error installing browsers: {e}")

# ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ«Ø¨ÙŠØª Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ (Ù„Ù„Ù…Ù†ØµØ§Øª Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠØ© Ù…Ø«Ù„ Streamlit Cloud)
if 'playwright_installed' not in st.session_state:
    with st.spinner("Ø¬Ø§Ø±ÙŠ ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø§Ù„Ø¨Ø­Ø« (Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ø¯Ù‚ÙŠÙ‚Ø© ÙÙŠ Ø§Ù„Ù…Ø±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰)..."):
        install_playwright_browsers()
        st.session_state['playwright_installed'] = True

# Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© NotImplementedError Ø¹Ù„Ù‰ ÙˆÙŠÙ†Ø¯ÙˆØ²
if sys.platform == 'win32':
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

def extract_emails_from_url(url):
    try:
        if not url or url == "N/A": return "N/A"
        if not url.startswith('http'): url = 'https://' + url
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø¹Ø¯Ø© ØµÙØ­Ø§Øª Ø´Ø§Ø¦Ø¹Ø©
        targets = [url]
        base_url = "/".join(url.split("/")[:3])
        # Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø© Ù„Ù„Ø§ØªØµØ§Ù„
        potential_pages = ['contact', 'contact-us', 'about', 'about-us', 'support', 'terms']
        for page_name in potential_pages:
            targets.append(f"{base_url}/{page_name}")
            targets.append(f"{base_url}/ar/{page_name}") # Ø¯Ø¹Ù… Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
            targets.append(f"{base_url}/en/{page_name}") # Ø¯Ø¹Ù… Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø± Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ØªØ±ØªÙŠØ¨
        targets = list(dict.fromkeys(targets))
        
        all_emails = set()
        for target in targets:
            try:
                response = requests.get(target, timeout=5, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'})
                if response.status_code == 200:
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¥ÙŠÙ…ÙŠÙ„Ø§Øª Ù…Ø¹ Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
                    found = re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', response.text)
                    for email in found:
                        if not any(ext in email.lower() for ext in ['.png', '.jpg', '.jpeg', '.gif', '.svg', 'wix', 'sentry']):
                            all_emails.add(email)
            except: continue
            if all_emails: break
            
        return ", ".join(list(all_emails)) if all_emails else "N/A"
    except:
        return "N/A"

def scrape_google_maps(search_query, max_results=10, data_placeholder=None, progress_bar=None):
    with sync_playwright() as p:
        results = []
        try:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø© Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø§ÙƒØªØ´Ø§Ù ÙˆÙ…Ø­Ø§ÙƒØ§Ø© Ø¥Ù†Ø³Ø§Ù†
            browser = p.chromium.launch(headless=True, args=[
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-blink-features=AutomationControlled"
            ])
            context = browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
                locale="ar-SA",
                viewport={'width': 1920, 'height': 1080}
            )
            page = context.new_page()
            
            # Ù…Ù†Ø¹ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ù…ØªØµÙØ­ ÙƒØ¢Ù„ÙŠ
            page.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            # 1. Ø§Ù„Ø°Ù‡Ø§Ø¨ Ù„Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ø£ÙˆÙ„Ø§Ù‹ Ù„ØªØ¶Ù„ÙŠÙ„ Ø£Ù†Ø¸Ù…Ø© Ø§Ù„ÙƒØ´Ù
            st.toast("ğŸ” Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¥Ù„Ù‰ Ø®Ø±Ø§Ø¦Ø· Ø¬ÙˆØ¬Ù„...")
            page.goto("https://www.google.com/maps?hl=ar", wait_until="domcontentloaded", timeout=60000)
            time.sleep(4)
            
            # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ®Ø·ÙŠ Ø£ÙŠ Ù†ÙˆØ§ÙØ° Ù…Ù†Ø¨Ø«Ù‚Ø© Ù„Ù„Ù…ÙˆØ§ÙÙ‚Ø©
            try:
                consent_btn = page.locator('button:has-text("Ù‚Ø¨ÙˆÙ„"), button:has-text("ÙˆØ§ÙÙ‚"), button:has-text("Accept")').first
                if consent_btn.is_visible():
                    consent_btn.click()
                    time.sleep(2)
            except: pass

            # 2. Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø±Ø¨Ø¹ Ø§Ù„Ø¨Ø­Ø« Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ø¨Ø§Ø´Ø±
            st.toast(f"ğŸ“ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù†: {search_query}")
            search_box = page.locator('#searchboxinput')
            search_box.wait_for(state="visible", timeout=20000)
            search_box.fill(search_query)
            page.keyboard.press("Enter")
            
            # Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ø­ØªÙ‰ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø£ÙˆÙ„ÙŠØ©
            time.sleep(6)

            seen_names = set()
            scroll_attempts = 0
            max_scroll_attempts = 60 
            
            while len(results) < max_results and scroll_attempts < max_scroll_attempts:
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù‡Ù†Ø§Ùƒ Ù†ØªÙŠØ¬Ø© ÙˆØ§Ø­Ø¯Ø© Ù…Ø¨Ø§Ø´Ø±Ø© (ØµÙØ­Ø© Ù…Ø¤Ø³Ø³Ø© Ù…ÙØªÙˆØ­Ø©)
                if page.locator('h1.DUwDvf').count() > 0:
                    name = page.locator('h1.DUwDvf').first.inner_text()
                    if name not in seen_names:
                        address = page.locator('button[data-item-id="address"]').first.inner_text() if page.locator('button[data-item-id="address"]').count() > 0 else "N/A"
                        phone = page.locator('button[data-item-id^="phone:tel:"]').first.inner_text() if page.locator('button[data-item-id^="phone:tel:"]').count() > 0 else "N/A"
                        website = page.locator('a[data-item-id="authority"]').first.get_attribute('href') if page.locator('a[data-item-id="authority"]').count() > 0 else "N/A"
                        
                        results.append({
                            "ğŸ¢ Ø§Ø³Ù… Ø§Ù„Ù…Ø¤Ø³Ø³Ø©": name, "ğŸ“ Ø±Ù‚Ù… Ø§Ù„Ù‡Ø§ØªÙ": phone, "ğŸŒ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø§Ù„ÙƒØªØ±ÙˆÙ†ÙŠ": website,
                            "ğŸ“§ Ø§Ù„Ø§ÙŠÙ…ÙŠÙ„": extract_emails_from_url(website) if website != "N/A" else "N/A",
                            "ğŸ“ Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…ÙƒØªØ¨": address
                        })
                        seen_names.add(name)
                        if data_placeholder: data_placeholder.dataframe(pd.DataFrame(results), use_container_width=True)
                        if max_results == 1: break # Ø¥Ø°Ø§ Ø·Ù„Ø¨ Ù†ØªÙŠØ¬Ø© ÙˆØ§Ø­Ø¯Ø© ÙˆØ¬Ø¯Ù†Ø§Ù‡Ø§

                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©
                item_selectors = ['.Nv262d', '.hfpxzc', 'a[href*="/maps/place/"]']
                items = []
                for sel in item_selectors:
                    found = page.locator(sel).all()
                    if len(found) > 0:
                        items = found
                        break
                
                if not items:
                    page.mouse.wheel(0, 2000)
                    time.sleep(3)
                    scroll_attempts += 1
                    continue

                for item in items:
                    if len(results) >= max_results: break
                    try:
                        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø£ÙˆÙ„ÙŠ Ù„Ù„ØªØ­Ù‚Ù‚
                        name_text = item.get_attribute("aria-label") or item.inner_text().split('\n')[0]
                        if not name_text or name_text in seen_names: continue

                        item.scroll_into_view_if_needed()
                        item.click(force=True, timeout=10000)
                        time.sleep(2) # Ø§Ù†ØªØ¸Ø§Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙØ§ØµÙŠÙ„
                        
                        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ù„ÙˆØ­Ø© Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ©
                        name_loc = page.locator('h1.DUwDvf')
                        if name_loc.count() > 0:
                            side_name = name_loc.first.inner_text()
                            if side_name in seen_names: continue
                            
                            address = page.locator('button[data-item-id="address"]').first.inner_text() if page.locator('button[data-item-id="address"]').count() > 0 else "N/A"
                            phone = page.locator('button[data-item-id^="phone:tel:"]').first.inner_text() if page.locator('button[data-item-id^="phone:tel:"]').count() > 0 else "N/A"
                            website = page.locator('a[data-item-id="authority"]').first.get_attribute('href') if page.locator('a[data-item-id="authority"]').count() > 0 else "N/A"
                            
                            results.append({
                                "ğŸ¢ Ø§Ø³Ù… Ø§Ù„Ù…Ø¤Ø³Ø³Ø©": side_name, "ğŸ“ Ø±Ù‚Ù… Ø§Ù„Ù‡Ø§ØªÙ": phone, "ğŸŒ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø§Ù„ÙƒØªØ±ÙˆÙ†ÙŠ": website,
                                "ğŸ“§ Ø§Ù„Ø§ÙŠÙ…ÙŠÙ„": extract_emails_from_url(website) if website != "N/A" else "N/A",
                                "ğŸ“ Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…ÙƒØªØ¨": address
                            })
                            seen_names.add(side_name)
                            if progress_bar: progress_bar.progress(len(results) / max_results)
                            if data_placeholder: data_placeholder.dataframe(pd.DataFrame(results), use_container_width=True)
                    except: continue
                
                # Ø§Ù„ØªÙ…Ø±ÙŠØ± Ù„Ø£Ø³ÙÙ„ Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø²ÙŠØ¯
                try:
                    feed = page.locator('div[role="feed"]')
                    if feed.count() > 0:
                        feed.evaluate("el => el.scrollBy(0, 4000)")
                    else:
                        page.mouse.wheel(0, 4000)
                except: page.mouse.wheel(0, 4000)
                
                time.sleep(3)
                scroll_attempts += 1
                if "reached the end" in page.content() or "Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©" in page.content(): break
                    
            browser.close()
            return results
        except Exception as e:
            if 'browser' in locals(): browser.close()
            return results

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©
st.set_page_config(page_title="Ù…Ø³ØªØ®Ø±Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø®Ø±Ø§Ø¦Ø· Ø¬ÙˆØ¬Ù„", layout="wide", initial_sidebar_state="expanded")

# ØªØµÙ…ÙŠÙ… Ø¹ØµØ±ÙŠ ÙˆØ£Ù†ÙŠÙ‚ Ù…Ø¹ ØªØ¬Ø§ÙˆØ² ØªÙ†Ø³ÙŠÙ‚Ø§Øª Streamlit
style_code = """<link href="https://fonts.googleapis.com/css2?family=Tajawal:wght@400;500;700&display=swap" rel="stylesheet"><style>
    body, .stApp { font-family: 'Tajawal', sans-serif !important; direction: RTL !important; text-align: right !important; background-color: #F8FAFC !important; }
    h1, h2, h3, p, span, label { font-family: 'Tajawal', sans-serif !important; text-align: right !important; color: #1E3A8A !important; }
    
    /* ØªÙ†Ø³ÙŠÙ‚ Ø´Ø±ÙŠØ· Ø§Ù„ØªÙ…Ø±ÙŠØ± Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ */
    [data-testid="stSidebar"] {
        background-color: #FFFFFF !important;
        border-left: 1px solid #E2E8F0 !important;
    }
    
    [data-testid="stSidebar"] .stMarkdown h3 {
        color: #2563EB !important;
        border-bottom: 2px solid #F1F5F9;
        padding-bottom: 10px;
    }

    /* ØªÙ†Ø³ÙŠÙ‚ Ù…Ø±Ø¨Ø¹Ø§Øª Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ */
    .stTextInput div[data-baseweb="input"], .stNumberInput div[data-baseweb="input"] {
        border: 1px solid #CBD5E1 !important;
        border-radius: 8px !important;
        background-color: white !important;
    }
    
    /* ØªÙ†Ø³ÙŠÙ‚ Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ - LTR */
    [data-testid="stDataFrame"], [data-testid="stTable"] {
        direction: LTR !important;
        text-align: left !important;
        background-color: white !important;
        border-radius: 12px !important;
        box-shadow: 0 1px 3px 0 rgba(0, 0, 0, 0.1) !important;
    }
    
    /* Ø¥Ø®ÙØ§Ø¡ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ù…Ø²Ø¹Ø¬Ø© */
    [data-testid="stInputHelperText"], .st-emotion-cache-1pxm8v5, .st-emotion-cache-10trblm { display: none !important; }
    
    /* ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø²Ø± */
    .stButton button {
        background-color: #2563EB !important;
        color: white !important;
        border-radius: 8px !important;
        padding: 0.6rem 1rem !important;
        width: 100% !important;
        font-weight: bold !important;
        font-family: 'Tajawal', sans-serif !important;
        border: none !important;
        transition: all 0.2s ease !important;
    }

    .stButton button p { color: white !important; }
    
    .stButton button:hover {
        background-color: #1E40AF !important;
        box-shadow: 0 4px 12px rgba(37, 99, 235, 0.2) !important;
    }
    
    # MainMenu, footer, header { visibility: hidden !important; }
    
    .developer-footer {
        position: fixed;
        bottom: 0;
        left: 0;
        width: 100%;
        background-color: #1E3A8A;
        color: white;
        text-align: center;
        padding: 8px 0;
        font-family: 'Tajawal', sans-serif;
        z-index: 100;
        font-size: 0.9rem;
    }
    </style>"""
st.markdown(style_code, unsafe_allow_html=True)

# Ø¥Ø¶Ø§ÙØ© Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ù…Ø·ÙˆØ± ÙÙŠ Ø§Ù„Ø£Ø³ÙÙ„
st.markdown("""
    <div class="developer-footer">
        ğŸ‘¨â€ğŸ’» ØªØ·ÙˆÙŠØ±: <b>Ø¹Ø¨Ø¯Ø§Ù„Ù…Ù†Ø¹Ù… Ø­Ø§ØªÙ…</b> | ğŸ“: +966544451878 | ğŸ“§: info@mohatim.tech
    </div>
    """, unsafe_allow_html=True)

st.title("ğŸ” Ù†Ø¸Ø§Ù… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø°ÙƒÙŠ")

# ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª ÙÙŠ Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ
with st.sidebar:
    st.markdown("### ğŸ› ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¨Ø­Ø«")
    business_type = st.text_input("Ù…Ø¬Ø§Ù„ Ø§Ù„Ù…Ø¤Ø³Ø³Ø©", placeholder="Ù…Ø·Ø§Ø¹Ù…ØŒ ÙÙ†Ø§Ø¯Ù‚...")
    city = st.text_input("Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©", placeholder="Ø§Ù„Ø±ÙŠØ§Ø¶ØŒ Ø¯Ø¨ÙŠ...")
    country = st.text_input("Ø§Ù„Ø¯ÙˆÙ„Ø©", placeholder="Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©...")
    max_res = st.number_input("Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©", min_value=1, max_value=500, value=10, step=1)
    
    st.markdown("---")
    search_clicked = st.button("ğŸš€ Ø§Ø¨Ø¯Ø£ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬")
    
    st.markdown("### ğŸ“– ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…")
    st.info("""
    1. Ø£Ø¯Ø®Ù„ Ù†ÙˆØ¹ Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„ØªØ¬Ø§Ø±ÙŠ.
    2. Ø­Ø¯Ø¯ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© ÙˆØ§Ù„Ø¯ÙˆÙ„Ø© Ø¨Ø¯Ù‚Ø©.
    3. Ø§Ø®ØªØ± Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ (Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ 500).
    4. Ø§Ø¶ØºØ· Ø¹Ù„Ù‰ Ø²Ø± Ø§Ù„Ø¨Ø¯Ø¡ ÙˆØ§Ù†ØªØ¸Ø± Ø§Ù„Ù†ØªØ§Ø¦Ø¬.
    5. ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨ØµÙŠØºØ© Word Ø£Ùˆ CSV.
    """)

def create_word_doc(data):
    doc = Document()
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù„Ø¯Ø¹Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
    style = doc.styles['Normal']
    font = style.font
    font.name = 'Arial'
    font.size = Pt(12)
    
    doc.add_heading('Ù†ØªØ§Ø¦Ø¬ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø®Ø±Ø§Ø¦Ø· Ø¬ÙˆØ¬Ù„', 0).alignment = WD_ALIGN_PARAGRAPH.RIGHT
    
    for entry in data:
        p = doc.add_paragraph()
        p.alignment = WD_ALIGN_PARAGRAPH.RIGHT
        p.add_run(f"ğŸ¢ Ø§Ø³Ù… Ø§Ù„Ù…Ø¤Ø³Ø³Ø©: ").bold = True
        p.add_run(f"{entry['ğŸ¢ Ø§Ø³Ù… Ø§Ù„Ù…Ø¤Ø³Ø³Ø©']}\n")
        
        p.add_run(f"ğŸ“ Ø±Ù‚Ù… Ø§Ù„Ù‡Ø§ØªÙ: ").bold = True
        p.add_run(f"{entry['ğŸ“ Ø±Ù‚Ù… Ø§Ù„Ù‡Ø§ØªÙ']}\n")
        
        p.add_run(f"ğŸŒ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø§Ù„ÙƒØªØ±ÙˆÙ†ÙŠ: ").bold = True
        p.add_run(f"{entry['ğŸŒ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø§Ù„ÙƒØªØ±ÙˆÙ†ÙŠ']}\n")
        
        p.add_run(f"ğŸ“§ Ø§Ù„Ø§ÙŠÙ…ÙŠÙ„: ").bold = True
        p.add_run(f"{entry['ğŸ“§ Ø§Ù„Ø§ÙŠÙ…ÙŠÙ„']}\n")
        
        p.add_run(f"ğŸ“ Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…ÙƒØªØ¨: ").bold = True
        p.add_run(f"{entry['ğŸ“ Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…ÙƒØªØ¨']}\n")
        
        doc.add_paragraph("-" * 30).alignment = WD_ALIGN_PARAGRAPH.RIGHT
    
    # Ø¥Ø¶Ø§ÙØ© Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ù…Ø·ÙˆØ± ÙÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù…Ù„Ù
    doc.add_paragraph("\n")
    dev_info = doc.add_paragraph()
    dev_info.alignment = WD_ALIGN_PARAGRAPH.CENTER
    run = dev_info.add_run("ØªÙ… Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙˆØ§Ø³Ø·Ø© Ù†Ø¸Ø§Ù… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø°ÙƒÙŠ")
    run.font.size = Pt(10)
    run.italic = True
    
    dev_contact = doc.add_paragraph()
    dev_contact.alignment = WD_ALIGN_PARAGRAPH.CENTER
    run2 = dev_contact.add_run("ØªØ·ÙˆÙŠØ±: Ø¹Ø¨Ø¯Ø§Ù„Ù…Ù†Ø¹Ù… Ø­Ø§ØªÙ… | Ø¬ÙˆØ§Ù„: +966544451878 | Ø§ÙŠÙ…ÙŠÙ„: info@mohatim.tech")
    run2.font.size = Pt(10)
    run2.bold = True

    bio = io.BytesIO()
    doc.save(bio)
    return bio.getvalue()

st.markdown("<br>", unsafe_allow_html=True)
if search_clicked:
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù‚ÙŠÙ…Ø© ÙˆØ§Ø­Ø¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ Ù„Ù„Ø¨Ø­Ø«
    if business_type or city or country:
        # Ø¨Ù†Ø§Ø¡ Ù†Øµ Ø§Ù„Ø¨Ø­Ø« Ø¨Ø´ÙƒÙ„ Ø£ÙƒØ«Ø± Ø¯Ù‚Ø© Ù„Ø¶Ù…Ø§Ù† ØªØµÙÙŠØ© Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© ÙˆØ§Ù„Ø¯ÙˆÙ„Ø©
        query_parts = []
        if business_type:
            query_parts.append(business_type)
        
        location_parts = []
        if city:
            location_parts.append(city)
        if country:
            location_parts.append(country)
            
        if location_parts:
            query_parts.append("ÙÙŠ")
            query_parts.append(", ".join(location_parts))
        
        query = " ".join(query_parts)
        st.info(f"Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù†: {query}...")
        
        progress_bar = st.progress(0)
        data_placeholder = st.empty()
        
        final_data = scrape_google_maps(query, max_res, data_placeholder, progress_bar)
        
        if final_data:
            st.success("Ø§ÙƒØªÙ…Ù„Øª Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬!")
            
            df = pd.DataFrame(final_data)
            
            # Ø¹Ø±Ø¶ Ø£Ø²Ø±Ø§Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„ ÙÙŠ Ø£Ø¹Ù…Ø¯Ø©
            col_word, col_csv = st.columns(2)
            
            with col_word:
                # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Word
                word_data = create_word_doc(final_data)
                st.download_button(
                    label="ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙƒÙ€ Word (.docx)",
                    data=word_data,
                    file_name="google_maps_results.docx",
                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                    use_container_width=True
                )
            
            with col_csv:
                # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù CSV
                csv = df.to_csv(index=False).encode('utf-8-sig')
                st.download_button(
                    label="ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙƒÙ€ CSV (.csv)",
                    data=csv,
                    file_name="google_maps_results.csv",
                    mime="text/csv",
                    use_container_width=True
                )
        else:
            st.error("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬.")
    else:
        st.warning("ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ù…Ø¹Ù„ÙˆÙ…Ø© ÙˆØ§Ø­Ø¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ Ù„Ù„Ø¨Ø­Ø« (Ø§Ù„Ù…Ø¬Ø§Ù„ Ø£Ùˆ Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© Ø£Ùˆ Ø§Ù„Ø¯ÙˆÙ„Ø©).")
